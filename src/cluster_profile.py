"""
File: src/cluster_profile.py
Mục tiêu:
  - Đảm bảo nhất quán phân cụm GMM k=5 giữa Page 03 & Page 04 (tránh mỗi lần mở app lại refit khác).
  - Fit 1 lần có chủ đích -> lưu artifacts -> lần sau chỉ "apply".

Artifacts (mặc định OUT_DIR = models/gmm/gmm_rfm_v1):
  model_gmm.pkl
  scaler.pkl
  labels.parquet                  (customer_id -> cluster_gmm + cluster_confidence)
  cluster_profile.parquet          (thống kê cụm)
  mapping_components.json          (component_id -> standardized_cluster_id)
  meta.json
  (optional) cluster_labels.json   (nếu truyền --label-json để áp tên marketing)

Chế độ chạy:
  1) Fit lần đầu:
     python src/cluster_profile.py fit --orders data/orders_full.csv
  2) Áp dụng lại (không refit):
     python src/cluster_profile.py apply --orders data/orders_full.csv
  3) Buộc refit (khi data đổi nhiều):
     python src/cluster_profile.py fit --force-refit
  4) Áp dụng + xuất labels khác:
     python src/cluster_profile.py apply --orders data/new_orders.csv --outdir models/gmm/gmm_rfm_v1

Gợi ý sửa Page 03:
  - Gỡ phần refit_gmm_k5 (refit trong giao diện).
  - Trước khi render, gọi load_artifacts() từ đây để lấy labels + profile -> join vào rfm.

Yêu cầu có các module:
  - src.rfm_base.load_orders, build_rfm_snapshot
  - src.rfm_rule_scoring.compute_rfm_scores
  - src.rfm_labeling.apply_rfm_level

Nếu thiếu sklearn -> cài đặt: pip install scikit-learn joblib

Tác giả: (Generated by assistant)
"""
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Tuple

import numpy as np
import pandas as pd
import joblib
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler

# ---- Import nội bộ (giống Page 03) ----
try:
    from src.rfm_base import load_orders, build_rfm_snapshot
    from src.rfm_rule_scoring import compute_rfm_scores
    from src.rfm_labeling import apply_rfm_level
except Exception as e:
    print(f"[ERROR] Không import được module nội bộ: {e}", file=sys.stderr)
    print("Hãy đảm bảo chạy từ root project (có thư mục src/).", file=sys.stderr)
    raise SystemExit(1)

TARGET_K = 5
DEFAULT_OUT_DIR = Path("models/gmm/gmm_rfm_v1")
DEFAULT_ORDERS_PATH = Path("data/orders_full.csv")
REQUIRED_RFM_FEATURES = ["Recency", "Frequency", "Monetary"]
RANDOM_STATE = 42

# --------------------------------------------------
# Utils
# --------------------------------------------------
def compute_data_hash(df: pd.DataFrame, feature_cols: list[str]) -> str:
    """
    Sinh hash nhẹ để phát hiện thay đổi dữ liệu đầu vào.
    Không nhằm bảo mật, chỉ so sánh thay đổi.
    """
    subset = df[feature_cols].copy()
    # Giảm độ dài: làm tròn số
    for c in subset.columns:
        subset[c] = subset[c].astype(float).round(4)
    # Sắp xếp theo index để ổn định
    subset = subset.sort_index()
    # Dùng pandas hash -> lấy hex rút gọn
    h = pd.util.hash_pandas_object(subset, index=True).values
    return hex(abs(int(h.sum())))[2:]


def standardize_cluster_ids(
    labels: np.ndarray,
    features_df: pd.DataFrame,
    rec_col: str,
    freq_col: str,
    mon_col: str
) -> Tuple[np.ndarray, Dict[int, int], pd.DataFrame]:
    """
    Chuẩn hóa ID cụm về 0..(k-1) theo Monetary_mean giảm dần.
    Trả về:
      - standardized_labels
      - mapping original_component -> standardized_id
      - agg bảng trung gian
    """
    tmp = features_df.copy()
    tmp["_comp"] = labels
    agg = (
        tmp.groupby("_comp")
           .agg({
               mon_col: "mean",
               freq_col: "mean",
               rec_col: "mean",
               "customer_id": "count"
           })
           .rename(columns={
               mon_col: "Monetary_mean",
               freq_col: "Frequency_mean",
               rec_col: "Recency_mean",
               "customer_id": "Count"
           })
    )
    ordered = agg.sort_values("Monetary_mean", ascending=False).index.tolist()
    mapping = {orig: new_id for new_id, orig in enumerate(ordered)}
    standardized = np.array([mapping[c] for c in labels])
    agg["StandardClusterID"] = [mapping[i] for i in agg.index]
    return standardized, mapping, agg.reset_index(drop=False).rename(columns={"_comp": "OriginalComponent"})


def build_cluster_profile(
    rfm_with_labels: pd.DataFrame,
    cluster_col: str = "cluster_gmm"
) -> pd.DataFrame:
    """
    Sinh bảng cluster_profile (thống kê sâu cho từng cụm).
    """
    df = rfm_with_labels.dropna(subset=[cluster_col])
    group = df.groupby(cluster_col)

    def q(series, p):
        try:
            return series.quantile(p)
        except Exception:
            return np.nan

    rows = []
    for cid, g in group:
        row = {
            "cluster_gmm": cid,
            "count": len(g),
            "Recency_mean": g["Recency"].mean(),
            "Recency_median": g["Recency"].median(),
            "Recency_std": g["Recency"].std(),
            "Recency_p25": q(g["Recency"], 0.25),
            "Recency_p75": q(g["Recency"], 0.75),
            "Frequency_mean": g["Frequency"].mean(),
            "Frequency_median": g["Frequency"].median(),
            "Frequency_std": g["Frequency"].std(),
            "Frequency_p25": q(g["Frequency"], 0.25),
            "Frequency_p75": q(g["Frequency"], 0.75),
            "Monetary_mean": g["Monetary"].mean(),
            "Monetary_median": g["Monetary"].median(),
            "Monetary_std": g["Monetary"].std(),
            "Monetary_p25": q(g["Monetary"], 0.25),
            "Monetary_p75": q(g["Monetary"], 0.75),
        }
        # Optional scores nếu tồn tại
        for sc in ["R", "F", "M"]:
            if sc in g.columns:
                row[f"{sc}_score_mean"] = g[sc].mean()
        rows.append(row)

    profile = pd.DataFrame(rows).set_index("cluster_gmm").sort_index()
    if profile["count"].sum() > 0:
        profile["share"] = profile["count"] / profile["count"].sum()
    else:
        profile["share"] = 0.0

    # Heuristic mô tả
    rec_med = rfm_with_labels["Recency"].median()
    freq_med = rfm_with_labels["Frequency"].median()
    mon_med = rfm_with_labels["Monetary"].median()

    def describe(r):
        tags = []
        tags.append("Fresh" if r["Recency_mean"] < rec_med else "Older")
        tags.append("HighFreq" if r["Frequency_mean"] > freq_med else "LowFreq")
        tags.append("HighValue" if r["Monetary_mean"] > mon_med else "LowValue")
        return " | ".join(tags)

    profile["cluster_label_desc"] = profile.apply(describe, axis=1)
    return profile


def load_labels_file(path: Path) -> pd.DataFrame:
    return pd.read_parquet(path)


# --------------------------------------------------
# Core Pipeline
# --------------------------------------------------
def build_rfm_dataframe(orders_path: Path) -> pd.DataFrame:
    """
    Dựa trên code Page 03: load raw -> build snapshot -> score -> apply level.
    """
    if not orders_path.exists():
        raise FileNotFoundError(f"Không tìm thấy file orders: {orders_path}")
    raw = load_orders(orders_path)
    rfm_base = build_rfm_snapshot(raw)
    rfm_score = compute_rfm_scores(rfm_base)
    rfm_final = apply_rfm_level(rfm_score)
    return rfm_final


def fit_gmm_and_save(
    rfm: pd.DataFrame,
    out_dir: Path,
    force_refit: bool = False,
    label_json: Optional[Path] = None
) -> None:
    """
    Fit GMM k=5, chuẩn hoá cluster id, lưu toàn bộ artifacts.
    Nếu out_dir đã có model + không force_refit -> cảnh báo và stop.
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    model_path = out_dir / "model_gmm.pkl"
    scaler_path = out_dir / "scaler.pkl"
    meta_path = out_dir / "meta.json"

    if model_path.exists() and not force_refit:
        print(f"[INFO] Model đã tồn tại tại {model_path}. Dùng --force-refit để ghi đè.")
        return

    missing = [c for c in REQUIRED_RFM_FEATURES if c not in rfm.columns]
    if missing:
        raise ValueError(f"Thiếu cột RFM bắt buộc: {missing}")

    # Khử NaN (nếu có)
    rfm_clean = rfm.dropna(subset=REQUIRED_RFM_FEATURES).copy()
    if len(rfm_clean) < len(rfm):
        print(f"[WARN] Loại {len(rfm) - len(rfm_clean)} dòng vì NaN trong RFM.")

    # Chuẩn bị features
    X = rfm_clean[REQUIRED_RFM_FEATURES].astype(float)
    if "customer_id" not in rfm_clean.columns:
        raise ValueError("Thiếu cột customer_id trong RFM.")
    features_df = X.copy()
    features_df["customer_id"] = rfm_clean["customer_id"].values

    # Scale
    scaler = StandardScaler()
    Xs = scaler.fit_transform(X.values)

    # Fit GMM
    gmm = GaussianMixture(
        n_components=TARGET_K,
        covariance_type="full",
        random_state=RANDOM_STATE,
        n_init=8
    )
    gmm.fit(Xs)

    # Predict & proba
    components = gmm.predict(Xs)
    probs = gmm.predict_proba(Xs)

    # Standardize
    std_labels, mapping, agg_tmp = standardize_cluster_ids(
        components,
        features_df[["customer_id", *REQUIRED_RFM_FEATURES]],
        "Recency", "Frequency", "Monetary"
    )

    # Build labels df
    labels_df = pd.DataFrame({
        "customer_id": features_df["customer_id"].values,
        "cluster_component": components,
        "cluster_gmm": std_labels,
        "cluster_confidence": probs.max(axis=1)
    }).set_index("customer_id")

    # Merge vào full RFM (giữ nguyên thứ tự)
    rfm_merged = rfm.set_index("customer_id").join(labels_df, how="left")

    # Build profile
    profile = build_cluster_profile(rfm_merged.reset_index(), cluster_col="cluster_gmm")

    # Optional cluster labels (marketing)
    if label_json and label_json.exists():
        try:
            label_map = json.loads(label_json.read_text(encoding="utf-8"))
            # label_map: {"0":"VIP","1":"Loyal",...}
            if isinstance(label_map, dict):
                profile["cluster_marketing_name"] = profile.index.map(
                    lambda x: label_map.get(str(x), label_map.get(int(x), f"Cluster {x}"))
                )
            else:
                print("[WARN] label_json không phải dict -> bỏ qua.")
        except Exception as e:
            print(f"[WARN] Không đọc được label_json: {e}")

    # Tính hash data
    data_hash = compute_data_hash(rfm_merged.reset_index(), REQUIRED_RFM_FEATURES)

    # Save artifacts
    labels_df.to_parquet(out_dir / "labels.parquet")
    profile.to_parquet(out_dir / "cluster_profile.parquet")
    joblib.dump(gmm, model_path)
    joblib.dump(scaler, scaler_path)

    # Save mapping component -> standardized id
    with open(out_dir / "mapping_components.json", "w", encoding="utf-8") as f:
        json.dump(mapping, f, indent=2, ensure_ascii=False)

    meta = {
        "model_type": "GMM",
        "n_components": TARGET_K,
        "standardized": True,
        "standardize_basis": "Monetary_mean_desc",
        "random_state": RANDOM_STATE,
        "features": REQUIRED_RFM_FEATURES,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "row_count": int(len(rfm_merged)),
        "data_hash": data_hash,
        "mapping_file": "mapping_components.json",
        "labels_file": "labels.parquet",
        "profile_file": "cluster_profile.parquet",
        "force_refit": force_refit
    }
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, indent=2, ensure_ascii=False)

    print(f"[OK] Đã fit & lưu model vào: {out_dir}")
    print(profile[["Recency_mean", "Frequency_mean", "Monetary_mean", "share"]])


def apply_existing_model(
    rfm: pd.DataFrame,
    out_dir: Path,
    label_json: Optional[Path] = None,
    write_outputs: bool = True
) -> None:
    """
    Dùng model + scaler đã lưu -> predict labels & profile cho dữ liệu (có thể mới).
    Không refit. Tôn trọng mapping cũ để giữ ID ổn định.
    """
    model_path = out_dir / "model_gmm.pkl"
    scaler_path = out_dir / "scaler.pkl"
    mapping_path = out_dir / "mapping_components.json"
    meta_path = out_dir / "meta.json"

    for p in [model_path, scaler_path, mapping_path, meta_path]:
        if not p.exists():
            raise FileNotFoundError(f"Thiếu artifact: {p}. Hãy chạy 'fit' trước.")

    gmm = joblib.load(model_path)
    scaler = joblib.load(scaler_path)
    mapping = json.loads(mapping_path.read_text(encoding="utf-8"))
    meta_old = json.loads(meta_path.read_text(encoding="utf-8"))

    missing = [c for c in REQUIRED_RFM_FEATURES if c not in rfm.columns]
    if missing:
        raise ValueError(f"Thiếu cột RFM bắt buộc: {missing}")

    rfm_clean = rfm.dropna(subset=REQUIRED_RFM_FEATURES).copy()
    if "customer_id" not in rfm_clean.columns:
        raise ValueError("Thiếu cột customer_id trong RFM.")

    X = rfm_clean[REQUIRED_RFM_FEATURES].astype(float)
    Xs = scaler.transform(X.values)

    components = gmm.predict(Xs)
    probs = gmm.predict_proba(Xs)

    # Map component -> standardized cluster id theo mapping cũ
    cluster_ids = np.array([mapping[str(c)] if str(c) in mapping else mapping.get(int(c), -1)
                            for c in components])

    labels_df = pd.DataFrame({
        "customer_id": rfm_clean["customer_id"].values,
        "cluster_component": components,
        "cluster_gmm": cluster_ids,
        "cluster_confidence": probs.max(axis=1)
    }).set_index("customer_id")

    rfm_merged = rfm.set_index("customer_id").join(labels_df, how="left")
    profile = build_cluster_profile(rfm_merged.reset_index(), cluster_col="cluster_gmm")

    # Optional marketing label
    if label_json and label_json.exists():
        try:
            label_map = json.loads(label_json.read_text(encoding="utf-8"))
            if isinstance(label_map, dict):
                profile["cluster_marketing_name"] = profile.index.map(
                    lambda x: label_map.get(str(x), label_map.get(int(x), f"Cluster {x}"))
                )
        except Exception as e:
            print(f"[WARN] Không đọc được label_json: {e}")

    data_hash_new = compute_data_hash(rfm_merged.reset_index(), REQUIRED_RFM_FEATURES)
    if data_hash_new != meta_old.get("data_hash"):
        print(f"[WARN] data_hash thay đổi (cũ={meta_old.get('data_hash')} / mới={data_hash_new}). "
              f"Bạn có thể cân nhắc refit nếu phân phối đã khác nhiều.")

    if write_outputs:
          labels_df.to_parquet(out_dir / "labels.parquet")
          profile.to_parquet(out_dir / "cluster_profile.parquet")
          # Ghi đè meta với updated_at (không đổi thông tin fit cũ)
          meta_old["last_apply_at"] = datetime.utcnow().isoformat() + "Z"
          meta_old["last_apply_row_count"] = int(len(rfm_merged))
          meta_old["last_apply_data_hash"] = data_hash_new
          with open(out_dir / "meta.json", "w", encoding="utf-8") as f:
              json.dump(meta_old, f, indent=2, ensure_ascii=False)

    print("[OK] Áp dụng model thành công. Preview profile:")
    print(profile[["Recency_mean", "Frequency_mean", "Monetary_mean", "share"]])


# --------------------------------------------------
# Public load function (Page 03 & Page 04 có thể dùng)
# --------------------------------------------------
def load_artifacts(out_dir: Path):
    """
    Trả về (model, scaler, labels_df, profile_df, meta, mapping)
    """
    model = joblib.load(out_dir / "model_gmm.pkl")
    scaler = joblib.load(out_dir / "scaler.pkl")
    labels = pd.read_parquet(out_dir / "labels.parquet")
    profile = pd.read_parquet(out_dir / "cluster_profile.parquet")
    meta = json.loads((out_dir / "meta.json").read_text(encoding="utf-8"))
    mapping = json.loads((out_dir / "mapping_components.json").read_text(encoding="utf-8"))
    return model, scaler, labels, profile, meta, mapping


# --------------------------------------------------
# CLI
# --------------------------------------------------
def parse_args():
    p = argparse.ArgumentParser(
        description="Quản lý phân cụm GMM RFM k=5 (fit/apply) – ổn định giữa các Page."
    )
    sub = p.add_subparsers(dest="command", required=True)

    # fit
    p_fit = sub.add_parser("fit", help="Fit mới (hoặc refit) model GMM k=5.")
    p_fit.add_argument("--orders", type=Path, default=DEFAULT_ORDERS_PATH,
                       help="Đường dẫn file orders đầu vào.")
    p_fit.add_argument("--outdir", type=Path, default=DEFAULT_OUT_DIR,
                       help="Thư mục lưu artifacts.")
    p_fit.add_argument("--force-refit", action="store_true",
                       help="Bỏ qua model cũ và refit.")
    p_fit.add_argument("--label-json", type=Path, default=None,
                       help="JSON map cluster_id -> tên marketing (vd: {'0':'VIP'}).")

    # apply
    p_apply = sub.add_parser("apply", help="Áp dụng model đã fit lên dữ liệu mới.")
    p_apply.add_argument("--orders", type=Path, default=DEFAULT_ORDERS_PATH,
                         help="File orders đầu vào.")
    p_apply.add_argument("--outdir", type=Path, default=DEFAULT_OUT_DIR,
                         help="Thư mục chứa artifacts model đã fit.")
    p_apply.add_argument("--label-json", type=Path, default=None,
                         help="Áp dụng tên marketing (nếu có).")
    p_apply.add_argument("--no-write", action="store_true",
                         help="Không ghi đè labels/profile/meta (chỉ preview).")

    return p.parse_args()


def main():
    args = parse_args()
    if args.command == "fit":
        rfm = build_rfm_dataframe(args.orders)
        fit_gmm_and_save(
            rfm=rfm,
            out_dir=args.outdir,
            force_refit=args.force_refit,
            label_json=args.label_json
        )
    elif args.command == "apply":
        rfm = build_rfm_dataframe(args.orders)
        apply_existing_model(
            rfm=rfm,
            out_dir=args.outdir,
            label_json=args.label_json,
            write_outputs=not args.no_write
        )
    else:
        print("Unknown command.")


if __name__ == "__main__":
    main()
